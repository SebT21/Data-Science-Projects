{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylBcREHc6LbJ"
      },
      "source": [
        "# ECE1508 Project - AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-FPMEol6LbK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.datasets as dsets\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6avdrwM6LbK"
      },
      "source": [
        "## Data Import\n",
        "\n",
        "I was using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qczdb8iP8BRw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lar_waSZ8K3e"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB7LXAxL8ODZ"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Xlyo1M8ShM"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d jonathanoheix/face-expression-recognition-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stxX3rEU8o2b"
      },
      "outputs": [],
      "source": [
        "!unzip face-expression-recognition-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-9_mQbBto9Q"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LWaoG556LbL"
      },
      "outputs": [],
      "source": [
        "#device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#import data\n",
        "data_dir = \"/content/images\"\n",
        "\n",
        "#transformations - AlexNet expects 224x224 RGB images\n",
        "transformations = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=3),  #convert B&W to 3-channel\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "}\n",
        "\n",
        "#load data\n",
        "data_sets = {\n",
        "    'train': dsets.ImageFolder(os.path.join(data_dir, 'train'), transform=transformations['train']),\n",
        "    'validation': dsets.ImageFolder(os.path.join(data_dir, 'validation'), transform=transformations['validation'])\n",
        "}\n",
        "\n",
        "#splitting training set to make a test set\n",
        "full_validation = data_sets['validation']\n",
        "val_size = int(0.5 * len(full_validation))\n",
        "test_size = len(full_validation) - val_size\n",
        "validation_dataset, test_dataset = random_split(full_validation, [val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "#create dataloaders\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data_sets['train'], batch_size=64, shuffle=True, num_workers=2),\n",
        "    'validation': DataLoader(validation_dataset, batch_size=64, shuffle=True, num_workers=2),\n",
        "    'test': DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd0fmFoI6LbM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5r5XUfJ6LbM"
      },
      "outputs": [],
      "source": [
        "#pre-trained AlexNet\n",
        "weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
        "model = models.alexnet(weights=weights)\n",
        "\n",
        "#change to 7 classes\n",
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 7)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1sDdPNQ6LbM"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-oFrl3C6LbM"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    #train and validation each epoch\n",
        "    for phase in ['train', 'validation']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #forward pass\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                #backward pass\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            #calcs\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        #copy if it's the best model\n",
        "        if phase == 'validation' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "#load best model\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(f'Best val Acc: {best_acc:4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898xZ_z-VUgB"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK3NwvyXVT73"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #forward pass\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        #update loss\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        #store predictions and true labels\n",
        "        all_preds.extend(preds.view(-1).cpu().numpy())\n",
        "        all_labels.extend(labels.view(-1).cpu().numpy())\n",
        "\n",
        "avg_loss = test_loss / len(dataloaders['test'].dataset)\n",
        "accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "#calculations\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}, '\n",
        "      f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j46J-oi99kb"
      },
      "source": [
        "## Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDsJ4K-p9__b"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0)  #add batch dimension\n",
        "    return image\n",
        "\n",
        "class AlexNet_Modified(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(AlexNet_Modified, self).__init__()\n",
        "        self.features1 = nn.Sequential(*list(original_model.features.children())[:3])  #up to first conv layer\n",
        "        self.features2 = nn.Sequential(*list(original_model.features.children())[3:6])  #up to second conv layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.features1(x)\n",
        "        x2 = self.features2(x1)\n",
        "        return x1, x2\n",
        "\n",
        "original_model = models.alexnet(pretrained=True)\n",
        "modified_model = AlexNet_Modified(model)\n",
        "modified_model = modified_model.to(device)\n",
        "\n",
        "image_path = '/content/images/train/angry/0.jpg'\n",
        "image = load_image(image_path).to(device)\n",
        "\n",
        "#ensure model is in evaluation mode\n",
        "modified_model.eval()\n",
        "\n",
        "#forward pass to get feature maps\n",
        "with torch.no_grad():\n",
        "    feature_maps1, feature_maps2 = modified_model(image)\n",
        "\n",
        "def visualize_feature_maps(feature_maps):\n",
        "    feature_maps = feature_maps.to('cpu').squeeze(0)  #remove batch dimension and move to CPU\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=min(5, feature_maps.size(0)), figsize=(15, 10))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(feature_maps[i].detach(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "#original image\n",
        "img = Image.open(image_path)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "#first feature map\n",
        "print(\"Feature Maps after 1st Conv Layer\")\n",
        "visualize_feature_maps(feature_maps1)\n",
        "\n",
        "#second feature map\n",
        "print(\"Feature Maps after 2nd Conv Layer\")\n",
        "visualize_feature_maps(feature_maps2)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
